---
layout : post
title : "2주차"
---

# 4장 모델 훈련
모델 훈련: 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것

## 4.1 선형 회귀
![image](https://github.com/user-attachments/assets/c62af4a6-42b2-471d-add8-2cdd23a0a2e3)

![image](https://github.com/user-attachments/assets/971ef7d6-af67-46f1-ab64-c9ffd2477800)

RMSE(평균 제곱근 오차): 회귀에서 가장 널리 사용되는 지표

MSE(평균 제곱 오차): RMSE와 같은 θ를 찾으며 더 간단함

![image](https://github.com/user-attachments/assets/a360ca26-441f-486c-b77b-4f5b3643506b)

### 4.1.1 정규 방정식
정규 방정식: 비용 함수를 최소화하는 θ를 찾기 위한 해석적인 방법

![image](https://github.com/user-attachments/assets/957154cb-d797-4d53-b886-a3133e963b1d)

연산자 @: 행렬 곱셈 수행

X+: X의 (무어-펜로즈) 유사역행렬

유사역행렬: 특잇값 분해(SVD)를 사용해 계산

Σ를 먼저 구함 -> 어떤 낮은 임곗값보다 작은 모든 수를 0으로 바꿈 -> 0이 아닌 모든 값을 역수로 치환 -> 행렬을 전치

```python
import numpy as np

np.random.seed(42) # 동일하게 재현되도록 하기 위해 지정합니다.
m = 100                               # 샘플 개수
X = 2 * np.random.rand(m, 1)          # 열 백터
y = 4 + 3 * X + np.random.randn(m, 1) # 열 백터
```

```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성
np.random.seed(42)  # 동일하게 재현되도록 하기 위해 지정합니다.
m = 100  # 샘플 개수
X = 2 * np.random.rand(m, 1)  # 열 백터
y = 4 + 3 * X + np.random.randn(m, 1)  # 열 백터

# 그래프 그리기
plt.scatter(X, y, c="blue", label="데이터")
plt.xlabel("X1")
plt.ylabel("y")
plt.title("랜덤으로 생성한 선형 데이터셋")
plt.legend()
plt.show()
```

```python
from sklearn.preprocessing import add_dummy_feature
import numpy as np

# 각 샘플에 x0 = 1을 추가
X_b = add_dummy_feature(X)

# 정규 방정식을 사용하여 theta_best 계산
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

# 결과 출력
print(theta_best)
```

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import add_dummy_feature

# 새로운 데이터 생성
X_new = np.array([[0], [2]])
X_new_b = add_dummy_feature(X_new)  # 각 샘플에 x0 = 1을 추가

# 예측 계산
y_predict = X_new_b @ theta_best
print(y_predict)

# 그래프 그리기
plt.plot(X_new, y_predict, "r-", label="예측")
plt.plot(X, y, "b.")
plt.xlabel("X1")
plt.ylabel("y")
plt.legend()
plt.show()
```

```python
# sklearn 사용
from sklearn.linear_model import LinearRegression

# 모델 초기화 및 학습
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# 회귀 계수와 절편 출력
print(lin_reg.intercept_, lin_reg.coef_)

# 새로운 데이터에 대한 예측
y_predict = lin_reg.predict(X_new)
print(y_predict)

# scipy의 lstsq() 함수 사용
import numpy as np

theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
print(theta_best_svd)

# np.linalg.pinv() 함수 사용
theta_best_pinv = np.linalg.pinv(X_b) @ y
print(theta_best_pinv)
```

### 4.1.2 계산 복잡도

정규 방정식은 (n+1)*(n+1) 크기의 (Xt)X의 역행렬을 계산

계산 복잡도: 일반적으로 특성 수가 두 배 늘어날 때 O(n의 2.4승=5.3) 에서 O(n의 3승=8) 사이로 계산 시간이 증가함.

## 4.2 경사 하강법

경사 하강법(Gradient Descent): 여러 종류의 문제엣 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘 -> 비용 함수 최소화의 반복하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상

학습률 하이퍼파라미터: 스텝의 크기, 너무 작으면 시간이 오래 걸림, 너무 크면 오히려 큰 값으로 발산할 수 있음

문제점: 전역 최솟값보다 덜 좋은 지역 최솟값에 수렴 가능(선형 회귀를 한 MSE 비용 함수는 볼록 함수여서 괜찮음)

다시 말해 모델 훈련은 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일

### 4.2.1 배치 경사 하강법

편도 함수(Partial Deriative): θj가 변경될 때 비용 함수가 얼마나 바뀌는지

![image](https://github.com/user-attachments/assets/9dfad2f4-f42c-40c4-bede-c2aef77d12c9)

![image](https://github.com/user-attachments/assets/dcb52970-1df7-4a59-989d-14c93bc4458d)

![image](https://github.com/user-attachments/assets/b5421d71-0bd1-4beb-9327-b08f4455bfed)

배치 경사 하강법: 매 스텝에서 훈련 데이터 전체를 사용하여 계산(계산 느림)

에포크: 훈련 세트를 한 번 반복하는 것

```python
import numpy as np

# 학습률 및 에포크 수 설정
eta = 0.1           # 학습률
n_epochs = 1000     # 반복 횟수

# 데이터 크기 설정
m = len(X_b)        # 샘플 개수

# 난수 초기화 및 가중치 초기화
np.random.seed(42)
theta = np.random.randn(2, 1)  # 모델 파라미터를 랜덤하게 초기화

# 경사 하강법을 통한 학습
for epoch in range(n_epochs):
    gradients = 2 / m * X_b.T @ (X_b @ theta - y)
    theta = theta - eta * gradients

# 최종 파라미터 출력
print(theta)
```

### 4.2.2 확률적 경사 하강법

확률적 경사 하강법: 매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 계산(무작위성으로 인한 불안정)

```python
import numpy as np

# 학습 스케줄 하이퍼파라미터 설정
n_epochs = 50
t0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터

# 학습 스케줄 함수 정의
def learning_schedule(t):
    return t0 / (t + t1)

# 난수 초기화 및 파라미터 초기화
np.random.seed(42)
theta = np.random.randn(2, 1)

# SGD(확률적 경사 하강법) 학습
for epoch in range(n_epochs):
    for iteration in range(m):
        random_index = np.random.randint(m)  # 무작위 샘플 선택
        xi = X_b[random_index:random_index + 1]
        yi = y[random_index:random_index + 1]
        gradients = 2 * xi.T @ (xi @ theta - yi)  # SGD에서는 m으로 나누지 않음
        eta = learning_schedule(epoch * m + iteration)  # 학습률 조정
        theta = theta - eta * gradients

# 결과 출력
print(theta)
```

```python
from sklearn.linear_model import SGDRegressor

# SGDRegressor 초기화
sgd_reg = SGDRegressor(
    max_iter=1000,  # 최대 반복 횟수
    tol=1e-5,       # 수렴 기준
    penalty=None,   # 정규화 항 사용 안 함
    eta0=0.01,      # 초기 학습률
    n_iter_no_change=100,  # 조기 종료를 위한 기준
    random_state=42  # 난수 고정
)

# 모델 학습
sgd_reg.fit(X, y.ravel())  # y.ravel()로 1D 배열 전달

# 모델 파라미터 출력
print("Intercept:", sgd_reg.intercept_)
print("Coefficients:", sgd_reg.coef_)
```

### 4.2.3 미니배치 경사 하강법

미니배치 경사 하강법: 각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그레이디언트를 계산하는 것이 아니라 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산(GPU를 사용해 성능 향상 가능)

![image](https://github.com/user-attachments/assets/fe4b4a8d-d1fb-49d9-9280-24c09b03210d)

## 4.3 다항 회귀

다항 회귀: 주어진 데이터가 직선보다 복잡한 형테일 때 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키는 기법

```python
import numpy as np
import matplotlib.pyplot as plt

# 난수 시드 고정 및 데이터 생성
np.random.seed(42)
m = 100  # 샘플 개수
X = 6 * np.random.rand(m, 1) - 3  # X 데이터 생성
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  # 비선형 데이터 생성

# 그래프 그리기
plt.scatter(X, y, color="blue", label="데이터")  # 데이터 산점도
plt.xlabel("X1")
plt.ylabel("y")
plt.title("잠음이 포함된 비선형 데이터셋")
plt.legend()
plt.show()
```

## 4.4 학습 곡선

과대적합: 훈련 데이터에서 성능이 좋지만, 교차 검증 점수가 나쁨. 모델이 너무 복잡

과소적합: 훈련 데이터 성능도 안좋고, 교차 검증 점수도 나쁨. 모델이 너무 단순

학습 곡선: 모델의 훈련 오차와 검증 오차를 훈련 반복 횟수의 함수로 나타낸 그래프
-> 훈련 세트와 검증 세트 사이에 공간이 있다는 것은 과대적합 모델

모델의 일반화 오차: 편향(잘못된 가정으로 인해 발생, 클수록 훈련 데이터에 과소적합), 분산(작은 변동에 과도하게 민감, 높을스록 훈련 데이터에 과대적합), 줄일 수 없는 오차(데이터 자체에 있는 잡음)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve

# 학습 곡선 데이터 계산
train_sizes, train_scores, valid_scores = learning_curve(
    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5, 
    scoring="neg_root_mean_squared_error"
)

# 평균 에러 계산
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

# 학습 곡선 그래프 그리기
plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="훈련 세트")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="검증 세트")
plt.xlabel("훈련 세트 크기")
plt.ylabel("RMSE")
plt.legend()
plt.grid(True)
plt.title("학습 곡선")
plt.show()
```

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve

# 데이터 생성
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)

# 다항 회귀 파이프라인 생성
polynomial_regression = make_pipeline(
    StandardScaler(),  # 데이터 정규화
    PolynomialFeatures(degree=10, include_bias=False),
    LinearRegression()
)

# 학습 곡선 데이터 계산
train_sizes, train_scores, valid_scores = learning_curve(
    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,
    scoring="neg_root_mean_squared_error"
)

# 평균 에러 계산
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

# 학습 곡선 그래프 그리기
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="훈련 세트")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="검증 세트")

# x축, y축 범위 및 눈금 설정
plt.xticks(np.arange(0, 81, 10))  # 훈련 세트 크기 10 단위
plt.yticks(np.arange(0.0, 2.6, 0.5))  # RMSE 범위 0.5 단위
plt.xlim(0, 80)
plt.ylim(0.0, 2.5)

# 그래프 설정
plt.xlabel("훈련 세트 크기", fontsize=12)
plt.ylabel("RMSE", fontsize=12)
plt.title("10차 다항 회귀의 학습 곡선", fontsize=14)
plt.legend(fontsize=10)
plt.grid(True)
plt.tight_layout()
plt.show()
```

## 4.5 규제가 있는 선형 모델

모델 규제(제한): 자유도를 줄여 과대적합을 줄임 

### 4.5.1 릿지 회귀

릿지 회귀: 규제가 추가된 선형 회귀 버전, 모델의 가중치가 가능한 한 작게 유지

하이퍼파라미터 α: 0이면 릿지 회귀=선형 회귀, 아주 크면 모든 가중치가 거의 0에 가까워짐

![image](https://github.com/user-attachments/assets/fe856570-0695-47d2-b781-a05a673230f6)

![image](https://github.com/user-attachments/assets/e5845258-c1ef-4cb3-894f-eb52613c5b82)

```python
from sklearn.linear_model import Ridge
import numpy as np

# 데이터 생성 (예제에서 사용된 X와 y가 필요)
# X와 y는 사전에 정의된 값이어야 합니다. 아래는 예제로 대체함.
X = np.random.rand(100, 1) * 10  # 예제 데이터
y = 0.5 * X + np.random.randn(100, 1)  # 예제 데이터

# 릿지 회귀 모델 생성
ridge_reg = Ridge(alpha=0.1, solver="cholesky")

# 모델 학습
ridge_reg.fit(X, y)

# 새로운 데이터로 예측
prediction = ridge_reg.predict([[1.5]])
print(prediction)
```

```python
from sklearn.linear_model import SGDRegressor
import numpy as np

# 데이터 생성 (예제에서 사용된 X와 y가 필요)
# X와 y는 사전에 정의된 값이어야 합니다. 아래는 예제로 대체함.
X = np.random.rand(100, 1) * 10  # 예제 데이터
y = 0.5 * X + np.random.randn(100, 1)  # 예제 데이터
m = len(X)  # 샘플 수

# 확률적 경사 하강법(SGD) 릿지 회귀 모델 생성
sgd_reg = SGDRegressor(
    penalty="l2",            # L2 정규화 사용
    alpha=0.1 / m,           # 정규화 강도 (릿지 회귀)
    tol=None,                # 조기 종료 조건 없음
    max_iter=1000,           # 최대 반복 횟수
    eta0=0.01,               # 초기 학습률
    random_state=42          # 난수 고정
)

# 모델 학습
sgd_reg.fit(X, y.ravel())  # y를 1차원 배열로 변환하여 전달

# 새로운 데이터로 예측
prediction = sgd_reg.predict([[1.5]])
print(prediction)
```

### 4.5.2 라쏘 회귀

라쏘 회귀: 선형 회귀의 또 다른 규제된 버전, 덜 중요한 특성의 가중치 제거

![image](https://github.com/user-attachments/assets/881d70c7-5ef1-4261-8b83-c5d7f46df84b)

서브그레이디언트 벡터: 미분이 불가능한 지점 근방 그레이디언트들의 중간값

![image](https://github.com/user-attachments/assets/710f8ba5-c166-42dc-a089-7de66115c4be)

```python
from sklearn.linear_model import Lasso
import numpy as np

# 데이터 생성 (예제 데이터)
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # X 데이터
y = 0.5 * X + np.random.randn(100, 1)  # y 데이터

# Lasso 회귀 모델 생성
lasso_reg = Lasso(alpha=0.1)

# 모델 학습
lasso_reg.fit(X, y)

# 새로운 데이터로 예측
prediction = lasso_reg.predict([[1.5]])
print(prediction)
```

### 4.5.3 엘라스틱넷

엘락스틱넷 회귀: 릿지 회귀와 라쏘 회귀를 절충한 모델, 규제항은 릿지와 라쏘를 혼합 비율 r을 사용해 더한 것

![image](https://github.com/user-attachments/assets/897b9621-f5f6-4a0c-b929-e84087980198)

```python
from sklearn.linear_model import ElasticNet
import numpy as np

# 데이터 생성 (예제 데이터)
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # X 데이터
y = 0.5 * X + np.random.randn(100, 1)  # y 데이터

# ElasticNet 모델 생성
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)

# 모델 학습
elastic_net.fit(X, y)

# 새로운 데이터로 예측
prediction = elastic_net.predict([[1.5]])
print(prediction)
```

### 4.5.4 조기 종료

조기 종료: 검증 오차가 최솟값에 도달하면 바로 훈련을 중지

```python
from copy import deepcopy
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDRegressor
import numpy as np

# 데이터 생성 (X_train, y_train, X_valid, y_valid)
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
split = int(m * 0.8)
X_train, X_valid = X[:split], X[split:]
y_train, y_valid = y[:split], y[split:]

# 데이터 전처리 및 모델 생성
preprocessing = make_pipeline(
    PolynomialFeatures(degree=90, include_bias=False),
    StandardScaler()
)
X_train_prep = preprocessing.fit_transform(X_train)
X_valid_prep = preprocessing.transform(X_valid)

sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)

# 학습 및 검증
n_epochs = 500
best_valid_rmse = float('inf')
best_model = None

for epoch in range(n_epochs):
    sgd_reg.partial_fit(X_train_prep, y_train.ravel())
    y_valid_predict = sgd_reg.predict(X_valid_prep)
    val_error = np.sqrt(mean_squared_error(y_valid, y_valid_predict))  # RMSE 계산
    if val_error < best_valid_rmse:
        best_valid_rmse = val_error
        best_model = deepcopy(sgd_reg)

# 결과 출력
print("최적의 검증 RMSE:", best_valid_rmse)
```

## 4.6 로지스틱 회귀

로지스틱(로짓) 회귀: 샘플이 특정 클래스에 속할 확률 추정

### 4.6.1 확률 추정

로지스틱 회귀 모델: 입력 특성의 가중치 합을 계산 후 편향을 더함. 바로 결과 출력을 하는 것이 아닌 결괏값의 로지스틱을 출력

![image](https://github.com/user-attachments/assets/0fae3f5d-d564-4a6d-a0a0-814d8dc16eb3)

![image](https://github.com/user-attachments/assets/533ec0c8-c1f1-442c-aefc-f7ae0b9f1ff0)

![image](https://github.com/user-attachments/assets/01aea738-8bcc-4bc0-bcd3-3b78f163caf7)


### 4.6.2 훈련과 비용 함수

훈련의 목적: 양성 샘플에 대해서는 높은 확률로 추정하고, 음성 샘플에 대해서는 낮은 확률을 추정하는 모델의 파라미터 벡터를 찾는 것

![image](https://github.com/user-attachments/assets/d73c4c82-1404-4dd4-bc3f-f1cd89855485)

![image](https://github.com/user-attachments/assets/4b3a112e-303c-4f26-8ed7-0a2938ecd38c)

![image](https://github.com/user-attachments/assets/40be95e7-5640-42db-861c-bf7053663b03)

### 4.6.3 결정경계

결정 경계(Decision Boundary): 양쪽의 확률이 똑같이 50%가 되는 근방

```python
from sklearn.datasets import load_iris
import pandas as pd

# Iris 데이터 로드
iris = load_iris(as_frame=True)

# 데이터셋 정보 확인
print("Iris 데이터 구성 요소:")
print(list(iris))

# 데이터셋 샘플 확인
print("\n데이터 샘플 (처음 3개):")
print(iris.data.head(3))  # 특성 데이터 (sepal length 등)

# 타겟 데이터 확인
print("\n타겟 데이터 (처음 3개):")
print(iris.target.head(3))

# 타겟 이름 확인
print("\n타겟 이름:")
print(iris.target_names)
```

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# 데이터 로드 및 분할
iris = load_iris(as_frame=True)
X = iris.data[["petal width (cm)"]].values
y = (iris.target_names[iris.target] == "virginica").astype(int)  # Virginica 여부를 이진 분류
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# 로지스틱 회귀 모델 학습
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# 새로운 데이터로 확률 계산
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # 0~3까지 1000개 샘플
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]  # 결정 경계

# 그래프 시각화
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Iris-Virginica가 아닐 확률")
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris-Virginica일 확률")
plt.plot([decision_boundary, decision_boundary], [0, 1], "k:", linewidth=2, label="결정 경계")
plt.text(decision_boundary + 0.02, 0.5, "결정 경계", fontsize=12, color="black", ha="center")

# 그래프 설정
plt.xlabel("꽃잎 너비(cm)", fontsize=12)
plt.ylabel("확률", fontsize=12)
plt.title("추정 확률과 결정 경계", fontsize=14)
plt.legend(fontsize=10)
plt.grid(True)
plt.tight_layout()
plt.show()
```

```python
# 결정 경계 출력
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]
print("decision_boundary:", decision_boundary)

# 특정 값에 대해 예측 수행
predictions = log_reg.predict([[1.7], [1.5]])
print("Predictions for [1.7], [1.5]:", predictions)
```

### 4.6.4 소프트맥스 회귀

소프트맥스 회귀(다항 로지스틱 회귀): 샘플 x가 주어지면 먼저 모델이 각 클래스 k에 대한 점수를 계산하고 소프트맥스 함수(정규화된 지수 함수)를 적용하여 각 클래스의 확률 추정

![image](https://github.com/user-attachments/assets/607aff51-a8c0-43d7-bbe0-c4f0ae99ecc8)

![image](https://github.com/user-attachments/assets/52168ce5-a919-45b6-9333-c32bb36b3510)

![image](https://github.com/user-attachments/assets/d01760e6-c26f-44ce-ab80-845cdc0cdb95)

argmax: 함수를 최대화하는 변수의 값을 반환

![image](https://github.com/user-attachments/assets/b9c4bc16-721c-4b44-970e-61b01485b28e)

![image](https://github.com/user-attachments/assets/ec49f8a2-8c44-4ac9-aa71-eed44cc72391)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# 데이터 로드
iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = iris["target"]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# 소프트맥스 회귀 모델 생성 및 학습
softmax_reg = LogisticRegression(C=30, random_state=42, multi_class="multinomial")
softmax_reg.fit(X_train, y_train)

# 학습 완료
print("Softmax Regression 모델 학습 완료")
```

```python
# 새로운 데이터로 예측 수행
sample = [[5, 2]]  # 꽃잎 길이 5cm, 너비 2cm

# 클래스 예측
predicted_class = softmax_reg.predict(sample)
print("Predicted class:", predicted_class)

# 클래스별 확률 예측
predicted_proba = softmax_reg.predict_proba(sample).round(2)
print("Class probabilities:", predicted_proba)
```
