---
layout : post
title : "3주차"
---

# 5장 서포트 벡터 머신

SVM은 다목적 머신러닝 모델로 중소규모의 비선형 데이터셋, 특히 분류 작업에서 빛을 발함. 특성의 스케일에 민감.

![image](https://github.com/user-attachments/assets/ac59d360-d0aa-44d7-94af-bf189b7cbe56)

## 5.1 선형 SVM 분류

SVM 분류 = 라지 마진 분류: 두 개의 클래스를 나누고 있으며 제일 가까운 훈련 샘플로부터 가능한 한 멀리 떨어져 있어 새로운 샘플에도 잘 작동하게 하는 분류

![image](https://github.com/user-attachments/assets/e03881dd-d996-4d5a-99f0-fcb5f62ca08a)

서포트 벡터: 도로 경계에 위치한 샘플

### 5.1.1 소프트 마진 분류

하드 마진 분류: 모든 샘플이 도로 바깥쪽에 올바르게 분류되어있는 상태

문제점1: 데이터가 선형적으로 구분될 수 있어야 제대로 작동함.

문제점2: 이상치에 민감함.

![image](https://github.com/user-attachments/assets/40994c2a-c2fd-44f4-aca5-b7594881f79a)

소프트 마진 분류: 도로의 폭을 가능한 한 넓게 유지 & 마진 오류 사이 적절한 균형 잡기

규제 하이퍼파라미터 C: 줄이면 도로가 커지지만, 더 많은 마진 오류 발생, 과대적합 위험 줄어듬

![image](https://github.com/user-attachments/assets/089ebf7a-5ad0-4063-8dc1-bac7b2ed1311)

```python
from sklearn.datasets import load_iris
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

# 아이리스 데이터셋 로드
iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values  # 꽃잎 길이 & 너비
y = (iris.target == 2)  # Iris-Virginica 여부 (True/False)

# SVM 분류기 생성 (표준화 포함)
svm_clf = make_pipeline(StandardScaler(),
                         LinearSVC(C=1, random_state=42))

# 모델 학습
svm_clf.fit(X, y)

# 새로운 샘플 예측
X_new = [[5.5, 1.7], [5.0, 1.5]]
y_pred = svm_clf.predict(X_new)

# 예측 결과 출력
print(y_pred)

# SVM 결정 함수 값 출력
decision_values = svm_clf.decision_function(X_new)

# 결과 출력
print(decision_values)
```

## 5.2 비선형 SVM 분류

선형적으로 분류할 수 없는 데이터셋은 다항 특성과 같은 특성을 더 추가하여 선형적으로 구분되는 데이터셋 만들기

![image](https://github.com/user-attachments/assets/2e8d94e9-6cd5-4908-aaa2-6b90dee57d40)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC

# 데이터 생성
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

# 다항 특성을 포함한 선형 SVM 분류기 생성
polynomial_svm_clf = make_pipeline(
    PolynomialFeatures(degree=3),
    StandardScaler(),
    LinearSVC(C=10, max_iter=10_000, random_state=42)
)

# 모델 학습
polynomial_svm_clf.fit(X, y)

# 결정 경계 시각화 함수
def plot_decision_boundary(clf, X, y):
    x1s = np.linspace(-1.5, 2.5, 100)
    x2s = np.linspace(-1, 1.5, 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_grid = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_grid).reshape(x1.shape)

    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap="coolwarm")
    plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker="^", label="Class 0", edgecolors="k")
    plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker="s", label="Class 1", edgecolors="k")
    plt.xlabel("$X_1$")
    plt.ylabel("$X_2$")
    plt.legend()
    plt.title("다항 특성을 사용한 선형 SVM 분류기")

# 결정 경계 시각화
plt.figure(figsize=(6, 4))
plot_decision_boundary(polynomial_svm_clf, X, y)
plt.show()
```

### 5.2.1 다항식 커널

커널 트릭: 실제로는 특성을 추가하지 않으면서 매우 높은 차수의 다항 특성을 많이 추가하는 것과 같은 결과를 얻게 해줌. 실제로 특성을 추가하지 않기에 엄청난 수의 특성 조합이 생김

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC

# 데이터 생성
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

# 두 개의 다항 커널 SVM 모델 정의
svm_clf_1 = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=3, coef0=1, C=5))
svm_clf_2 = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=10, coef0=100, C=5))

# 모델 학습
svm_clf_1.fit(X, y)
svm_clf_2.fit(X, y)

# 결정 경계 시각화 함수
def plot_decision_boundary(clf, X, y, title, ax):
    x1s = np.linspace(-1.5, 2.5, 100)
    x2s = np.linspace(-1, 1.5, 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_grid = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_grid).reshape(x1.shape)

    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap="coolwarm")
    ax.scatter(X[y==0][:, 0], X[y==0][:, 1], marker="^", label="Class 0", edgecolors="k")
    ax.scatter(X[y==1][:, 0], X[y==1][:, 1], marker="s", label="Class 1", edgecolors="k")
    ax.set_xlabel("$X_1$")
    ax.set_ylabel("$X_2$")
    ax.set_title(title)

# 두 개의 서브플롯 생성
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

plot_decision_boundary(svm_clf_1, X, y, "degree=3, coef0=1, C=5", axes[0])
plot_decision_boundary(svm_clf_2, X, y, "degree=10, coef0=100, C=5", axes[1])

plt.tight_layout()
plt.show()
```

### 5.2.2 유사도 특성

특정 랜드마크와 얼마나 닮았는지 측정하는 '유사도 함수'로 계산한 특성을 추가

![image](https://github.com/user-attachments/assets/8332e047-e7e8-42f0-a4dd-a2e8be8f4c0d)

랜드마크 선택하는 간단한 방법은 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정한는 것

장점: 차원이 매우 커져 변환된 훈련 세트가 선형적으로 구분될 가능성 높음

단점: 훈련 세트의 n개 특성을 가진 m개 샘플이 m개 특성을 가진 m개 샘플로 변환

### 5.2.3 가우스 RBF 커널

커널 트릭을 사용해 유사도 특성을 많이 추가하는 것과 같은 비슷한 결과를 얻음

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC

# 데이터 생성
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

# 네 개의 RBF 커널 SVM 모델 정의 (gamma, C 조합)
svm_clf_1 = make_pipeline(StandardScaler(), SVC(kernel="rbf", gamma=0.1, C=0.001))
svm_clf_2 = make_pipeline(StandardScaler(), SVC(kernel="rbf", gamma=0.1, C=1000))
svm_clf_3 = make_pipeline(StandardScaler(), SVC(kernel="rbf", gamma=5, C=0.001))
svm_clf_4 = make_pipeline(StandardScaler(), SVC(kernel="rbf", gamma=5, C=1000))

# 모델 학습
svm_clf_1.fit(X, y)
svm_clf_2.fit(X, y)
svm_clf_3.fit(X, y)
svm_clf_4.fit(X, y)

# 결정 경계 시각화 함수
def plot_decision_boundary(clf, X, y, title, ax):
    x1s = np.linspace(-1.5, 2.5, 100)
    x2s = np.linspace(-1, 1.5, 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_grid = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_grid).reshape(x1.shape)

    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap="coolwarm")
    ax.scatter(X[y==0][:, 0], X[y==0][:, 1], marker="^", label="Class 0", edgecolors="k")
    ax.scatter(X[y==1][:, 0], X[y==1][:, 1], marker="s", label="Class 1", edgecolors="k")
    ax.set_xlabel("$X_1$")
    ax.set_ylabel("$X_2$")
    ax.set_title(title)

# 네 개의 서브플롯 생성
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

plot_decision_boundary(svm_clf_1, X, y, "gamma=0.1, C=0.001", axes[0, 0])
plot_decision_boundary(svm_clf_2, X, y, "gamma=0.1, C=1000", axes[0, 1])
plot_decision_boundary(svm_clf_3, X, y, "gamma=5, C=0.001", axes[1, 0])
plot_decision_boundary(svm_clf_4, X, y, "gamma=5, C=1000", axes[1, 1])

plt.tight_layout()
plt.show()
```

어떤 커널은 특정 데이터 구조에 특화

ex) 문자열 커널(문자열 서브시퀀스 커널 or 레벤 슈타인 거리 기반의 커널): 텍스트 문서나 DNA 서열 분류

여러 가지 커널 중 선형 커널을 가장 먼저 시도하는 것이 나음

### 5.2.4 계산 복잡도

1. LinearSVC: 훈련 시간 복잡도는 대략 O(m*n) 정도

-> 정밀도를 높이면 수행 시간 길어짐

2. SVC: 훈련 복잡도는 O(m^2*n) ~ O(m^3*n)

-> 훈련 샘플 수가 커지면 엄청나게 느려지지만, 희소 특성인 경우 잘 확장

3. SGD Classifier: 훈련 시간 복잡도는 O(m*n)

![image](https://github.com/user-attachments/assets/556d04bb-44d6-4401-b0ef-8375630b67eb)

## 5.3 SVM 회귀

SVM 회귀: 제한된 마진 오류 안에서 도로 안에 가능한 한 많은 샘플이 들어가도록 학습

도로의 폭은 하이퍼파라미터 ε로 조절

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# 데이터 생성
np.random.seed(42)
m = 100
X = 2 * np.random.rand(m, 1)
y = 4 + 3 * X + np.random.randn(m, 1)  # 선형 데이터에 노이즈 추가

# 두 개의 SVR 모델 정의 (epsilon 값 비교)
svm_reg_1 = make_pipeline(StandardScaler(), LinearSVR(epsilon=0.5, random_state=42))
svm_reg_2 = make_pipeline(StandardScaler(), LinearSVR(epsilon=1.2, random_state=42))

# 모델 학습
svm_reg_1.fit(X, y.ravel())
svm_reg_2.fit(X, y.ravel())

# 예측값 계산을 위한 X 범위 설정
X_new = np.linspace(0, 2, 100).reshape(-1, 1)
y_pred_1 = svm_reg_1.predict(X_new)
y_pred_2 = svm_reg_2.predict(X_new)

# 그래프 시각화
fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)

# 첫 번째 그래프 (epsilon=0.5)
axes[0].scatter(X, y, color="blue", marker="o", edgecolors="black", alpha=0.7)
axes[0].plot(X_new, y_pred_1, "k-", linewidth=2, label=r"$\hat{y}$")
axes[0].plot(X_new, y_pred_1 + 0.5, "k--", linewidth=1, label=r"$\epsilon$")
axes[0].plot(X_new, y_pred_1 - 0.5, "k--", linewidth=1)
axes[0].set_title(r"epsilon=0.5")
axes[0].set_xlabel("$X_1$")
axes[0].set_ylabel("$y$")
axes[0].legend()

# 두 번째 그래프 (epsilon=1.2)
axes[1].scatter(X, y, color="blue", marker="o", edgecolors="black", alpha=0.7)
axes[1].plot(X_new, y_pred_2, "k-", linewidth=2, label=r"$\hat{y}$")
axes[1].plot(X_new, y_pred_2 + 1.2, "k--", linewidth=1, label=r"$\epsilon$")
axes[1].plot(X_new, y_pred_2 - 1.2, "k--", linewidth=1)
axes[1].set_title(r"epsilon=1.2")
axes[1].set_xlabel("$X_1$")
axes[1].legend()

plt.tight_layout()
plt.show()
```

## 5.4 SVM 이론

선형 SVM 분류기 모델은 단순히 결정 함수를 계산해서 새로운 샘플 x의 클래스를 예측함

결괏값이 0보다 크면 양성 클래스(1), 그렇지 않으면 음성 클래스(0)

![image](https://github.com/user-attachments/assets/5f944505-e550-4ed1-a5d2-0d1499853a9c)

![image](https://github.com/user-attachments/assets/43702154-2dd4-463c-af83-14e7f8b80f0e)

소프트 마진 분류기의 목적 함수를 구성하려면 각 샘플에 대해 슬랙변수>=0을 도입

이때 슬랙 변수는 i번쨰 샘플이 얼마나 마진을 위반할지 결정

![image](https://github.com/user-attachments/assets/ee7f9ba3-a457-4039-b0c6-3608a3725692)

콰드라틱 프로그래밍 문제: 하드&소프트 마진 문제와 같이 선형적인 제약 조건이 있는 볼록 함수의 이차 최적화 문제

SVM 훈련 방법1: QP 솔버 사용

SVM 훈련 방법2: 힌지 손실 또는 제곱 힌지 손실 최소화

![image](https://github.com/user-attachments/assets/0fe08fdc-2540-4bce-9c26-7a337c9ce311)

## 5.5 쌍대 문제

쌍대 문제: 원 문제라고 하는 제약이 있는 최적화 문제가 주어질 때 다르게 표현

이때 쌍대 문제의 해는 원 문제 해의 하한 값이지만 어떤 조건하에서는 원 문제와 똑같은 해를 제공

![image](https://github.com/user-attachments/assets/d5bb3b2a-0bb0-4799-9ad2-fe4693de4ae0)

![image](https://github.com/user-attachments/assets/17386f38-d56a-4dc1-a095-9ccf0b45c7f7)

훈련 샘플 수가 특성 개수보다 작을 때 원 문제보다 쌍대 문제를 푸는 것이 더 빠름

### 5.5.1 커널 SVM

![image](https://github.com/user-attachments/assets/2f95674a-62be-4cfd-92fb-c5ae1575a3b9)

![image](https://github.com/user-attachments/assets/f5ce6dba-ec33-4826-b58a-e4ce97fc84e2)

실제로 훈련 샘플을 변환할 필요가 전혀 없이 점곱을 제곱으로 바꾸기만 하면 됨

![image](https://github.com/user-attachments/assets/efad16b4-88a9-46e0-8187-f953df0f824b)

![image](https://github.com/user-attachments/assets/1ba288a4-b534-428f-8ed1-acad34dffd32)

![image](https://github.com/user-attachments/assets/77884cc2-c801-43dc-a87d-7822601680d4)
