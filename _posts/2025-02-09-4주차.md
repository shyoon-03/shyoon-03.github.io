---
layout : post
title : "4주차"
---

# 7장 앙상블 학습과 랜덤 포레스트

대중의 지혜: 랜덤 선택된 수천 명의 사람의 답 > 전문가의 답

앙상블 학습: 일련의 예측기(앙상블)로부터 예측을 수집 > 가장 좋은 모델 하나

랜덤 포레스트: 결정 트리의 앙상블

--->>>앙상블 방법은 대부분의 머신러닝 작업에서 가장 먼저 테스트하며

특히 서로 다른 종류로 구성된 표 형식 데이터에서 빛을 발함

또한 전처리가 거의 필요하지 않기 때문에 프로토타입을 빠르게 구축하는 데 적합

마지막으로 투표 기반 분류기와 스태킹 분류기 같은 앙상블 방법은 시스템 성능을 한계까지 끌어올리는데 도움이 됨

## 7.1 투표 기반 분류기

직접 투표 분류기: 각 분류기의 예측을 집계하여 다수결 투표로 정함.

-> 큰 수의 법칙으로 인해 약한 학습기가 충분하게 많고 다양하면 강한 학습기가 될 수 있음.

```python
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# 🌟 1. moons 데이터셋 생성
X, y = make_moons(n_samples=500, noise=0.30, random_state=42)

# 🌟 2. 훈련 세트와 테스트 세트 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# 🌟 3. 투표 기반 분류기 생성 (3개의 서로 다른 모델 조합)
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),  # 로지스틱 회귀
        ('rf', RandomForestClassifier(random_state=42)),  # 랜덤 포레스트
        ('svc', SVC(random_state=42))  # 서포트 벡터 머신 (SVM)
    ]
)

# 🌟 4. 모델 학습
voting_clf.fit(X_train, y_train)

# 🌟 5. 학습 완료 메시지 출력
print("Voting Classifier Training Complete! ✅")
```

```python
# 개별 분류기의 정확도 평가
for name, clf in voting_clf.named_estimators_.items():
    print(name, "=", clf.score(X_test, y_test))

# 투표 기반 분류기의 예측
print(voting_clf.predict(X_test[:1]))

# 개별 분류기의 예측 결과 확인
print([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])

# 투표 기반 분류기의 성능 평가
print(voting_clf.score(X_test, y_test))
```

```python
# 소프트 투표(Soft Voting) 방식으로 변경
voting_clf.voting = "soft"

# SVC 모델의 확률 예측 활성화
voting_clf.named_estimators_["svc"].probability = True

# 다시 모델 학습
voting_clf.fit(X_train, y_train)

# 투표 기반 분류기의 성능 평가
print(voting_clf.score(X_test, y_test))
```

-> 오류 수정

```python
# 1. 새로운 SVC 모델을 생성하면서 probability=True 설정
svc_clf = SVC(probability=True, random_state=42)

# 2. 기존 voting_clf를 수정해서 새로운 SVC 모델을 포함하도록 변경
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),  
        ('rf', RandomForestClassifier(random_state=42)),  
        ('svc', svc_clf)  # 수정된 SVC 모델 추가
    ],
    voting="soft"  # 소프트 투표 방식으로 설정
)

# 3. 모델 재학습
voting_clf.fit(X_train, y_train)

# 4. 성능 평가
print(voting_clf.score(X_test, y_test))
```

## 7.2 배깅과 페이스팅

다양한 분류기 만드는 방법1: 각기 다른 훈련 알고리즘을 사용하는 것

다양한 분류기 만드는 방법2: 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습시키는 것

배깅: 훈련 세트에서 중복을 허용하여 샘플링

페이스팅: 훈련 세트에서 중복을 허용하지 않고 샘플링

집계함수: 분류 시에는 통계적 최빈값, 회귀 시에는 평균

### 7.2.1 사이킷런의 배깅과 페이스팅

배깅은 각 예측기가 학습하는 서브셋에 다양성을 추가하므로 배깅이 페이스팅보다 편향이 조금 더 높음.

하지만 다양성을 추가한다는 것은 예측기들의 상관관계를 줄이므로 앙상블의 분산이 줄어듬.

전반적으로 배깅이 더 나은 모델을 만들기 때문에 일반적으로 더 선호

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# 배깅 분류기 생성
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
    n_estimators=500,      # 500개의 결정 트리
    max_samples=100,       # 각 트리에 100개의 샘플 사용
    n_jobs=-1,             # 모든 CPU 코어 사용
    random_state=42        # 랜덤 시드 설정
)

# 모델 학습
bag_clf.fit(X_train, y_train)
```

### 7.2.2 OOB 평가

OOB(Out Of Bag): 각 예측기에 선택되지 않은 나머지 훈련 샘플

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 배깅 분류기 생성 (OOB 평가 포함)
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
    n_estimators=500,      # 500개의 결정 트리 사용
    oob_score=True,        # OOB 샘플을 이용한 평가 활성화
    n_jobs=-1,             # 모든 CPU 코어 사용
    random_state=42        # 랜덤 시드 설정
)

# 모델 학습
bag_clf.fit(X_train, y_train)

# OOB 평가 점수 출력
print("OOB Score:", bag_clf.oob_score_)

# 테스트 세트에서 정확도 평가
y_pred = bag_clf.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred))

# OOB 결정 함수 값 확인
print("OOB Decision Function (첫 3개 샘플):", bag_clf.oob_decision_function_[:3])
```

## 7.3 랜덤 패치와 랜덤 서브스페이스

랜덤 패치 방식: 훈련 특성과 샘플을 모두 샘플링

랜덤 서브스페이스 방식: 훈련 샘플을 모두 사용하고 특성을 샘플링

## 7.4 랜덤 포레스트

랜덤 포레스트: 일반적으로 배깅 방법(또는 페이스팅)을 적용한 결정 트리의 앙상블

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# 랜덤 포레스트 분류기 생성
rnd_clf = RandomForestClassifier(
    n_estimators=500,  # 500개의 트리 사용
    max_leaf_nodes=16,  # 최대 리프 노드 개수 제한
    n_jobs=-1,  # 모든 CPU 코어 사용
    random_state=42  # 랜덤 시드 설정
)

# 모델 학습
rnd_clf.fit(X_train, y_train)

# 예측 수행
y_pred_rf = rnd_clf.predict(X_test)

# 배깅을 이용한 랜덤 포레스트와 동일한 모델 생성
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
    n_estimators=500,  # 500개의 트리 사용
    n_jobs=-1,  # 모든 CPU 코어 사용
    random_state=42  # 랜덤 시드 설정
)

# 배깅 모델 학습
bag_clf.fit(X_train, y_train)
```

### 7.4.1 엑스트라 트리

각 노드는 랜덤으로 특성의 서브셋을 만들어 분할에 사용

익스트림 랜덤 트리(=엑스트라 트리): 극단적으로 랜덤한 트리, 편향이 늘어나는 대신 분산이 낮아짐

### 7.4.2 특성 중요도

랜덤 포레스트의 장점: 특성의 상대적 중요도를 측정하기 쉬움

-> 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

# 아이리스 데이터셋 로드
iris = load_iris(as_frame=True)

# 랜덤 포레스트 모델 생성 및 학습
rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)
rnd_clf.fit(iris.data, iris.target)

# 특성 중요도 출력
for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):
    print(round(score, 2), name)
```

## 7.5 부스팅

부스팅(=가설 부스팅): 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법

앞의 모델을 보완해 나가면서 일련의 예측기를 학습시킴

### 7.5.1 AdaBoost

이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높임

-> 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨

가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측기마다 다른 가중치가 적용됨

![image](https://github.com/user-attachments/assets/500bbcec-b48b-4ab8-b4bc-fbbcc6c10105)

![image](https://github.com/user-attachments/assets/3740491b-bf1b-4d1e-a7e5-180a8d0c131d)

![image](https://github.com/user-attachments/assets/8f0332f4-a299-47a7-a223-9bc4eafe5cf3)

![image](https://github.com/user-attachments/assets/28ea13cd-e53a-45b8-b67f-49bba8dcd6c4)

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# AdaBoost 모델 생성 및 학습
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1),  # 약한 학습기 (깊이가 1인 결정 트리)
    n_estimators=30,  # 트리 개수
    learning_rate=0.5,  # 학습률
    random_state=42
)

# 모델 학습
ada_clf.fit(X_train, y_train)
```

### 7.5.2 그레이디언트 부스팅

이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor

# 난수 초기화
np.random.seed(42)

# 데이터 생성
X = np.random.rand(100, 1) - 0.5
y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + 가우스 잡음

# 첫 번째 결정 트리 회귀 모델
tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)

# 첫 번째 예측값 생성
y2 = y - tree_reg1.predict(X)

# 두 번째 결정 트리 회귀 모델
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)
tree_reg2.fit(X, y2)

# 두 번째 예측값 생성
y3 = y2 - tree_reg2.predict(X)

# 세 번째 결정 트리 회귀 모델
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)
tree_reg3.fit(X, y3)

# 새로운 샘플 예측
X_new = np.array([[-0.4], [0.], [0.5]])
predictions = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
print(predictions)
```

```python
from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boosting Regressor 모델 생성 및 학습
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,
                                 learning_rate=1.0, random_state=42)
gbrt.fit(X, y)
```

축소: 기여도를 낮게 설정하는 규제 방법 

-> 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만, 일반적으로 예측의 성능은 좋아짐

'''python
from sklearn.ensemble import GradientBoostingRegressor

gbrt_best = GradientBoostingRegressor(
    max_depth=2,
    learning_rate=0.05,
    n_estimators=500,
    n_iter_no_change=10,
    random_state=42
)

gbrt_best.fit(X, y)

print(gbrt_best.n_estimators_) # 최적의 트리 개수 확인
'''

확률적 그레이디언트 부스팅: 각 트리가 훈련할 때 사용할 훈련 샘플의 비용을 지정

### 7.5.3 히스토그램 기반 그레이디언트 부스팅

입력 특성을 구간으로 나누어 정수로 대체하는 방식으로 작동

구간 분할을 사용하면 학습 알고리즘이 평가해야 하는 임곗값의 수를 크게 줄일 수 있음

또한 정수로 작업하면 더 빠르고 메모리 효율적인 데이터 구조를 사용할 수 있음

정밀도 손실을 유발하므로 데이터셋에 따라 과대적합을 줄이는 데 도움이 될 수도 과소적합을 유발할 수도 있음

```python
from sklearn.datasets import fetch_california_housing
import pandas as pd

# 데이터 불러오기
housing_data = fetch_california_housing(as_frame=True)
housing = housing_data.frame
housing_labels = housing["MedHouseVal"]  # 타깃 변수 (집 값)
housing = housing.drop("MedHouseVal", axis=1)  # 입력 변수만 남김

# ocean_proximity 변수를 가정 (캘리포니아 주택 데이터에는 없음, 예제용 추가)
housing["ocean_proximity"] = pd.cut(housing["AveRooms"],
                                    bins=[0, 2, 5, 10, float("inf")],
                                    labels=["near", "mid", "far", "very_far"])


from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.preprocessing import OrdinalEncoder

hgb_reg = make_pipeline(
    make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]),
                            remainder="passthrough"),
    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)
)

hgb_reg.fit(housing, housing_labels)
```

## 7.6 스태킹

앙상블에 속한 모든 예측기의 예측을 취합하는 모델

블렌더(=메타 학습기): 마지막 예측기

![image](https://github.com/user-attachments/assets/175a8871-d460-4b59-9e2a-1be89bd73b1f)

![image](https://github.com/user-attachments/assets/d40a9df4-314a-443e-9e2c-83dc31fef369)

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

stacking_clf = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),
        ('rf', RandomForestClassifier(random_state=42)),
        ('svc', SVC(probability=True, random_state=42))
    ],
    final_estimator=RandomForestClassifier(random_state=43),
    cv=5  # 교차 검증 폴드 개수
)

stacking_clf.fit(X_train, y_train)
```
