---
layout : post
title : "4ì£¼ì°¨"
---

# 7ì¥ ì•™ìƒë¸” í•™ìŠµê³¼ ëœë¤ í¬ë ˆìŠ¤íŠ¸

ëŒ€ì¤‘ì˜ ì§€í˜œ: ëœë¤ ì„ íƒëœ ìˆ˜ì²œ ëª…ì˜ ì‚¬ëŒì˜ ë‹µ > ì „ë¬¸ê°€ì˜ ë‹µ

ì•™ìƒë¸” í•™ìŠµ: ì¼ë ¨ì˜ ì˜ˆì¸¡ê¸°(ì•™ìƒë¸”)ë¡œë¶€í„° ì˜ˆì¸¡ì„ ìˆ˜ì§‘ > ê°€ì¥ ì¢‹ì€ ëª¨ë¸ í•˜ë‚˜

ëœë¤ í¬ë ˆìŠ¤íŠ¸: ê²°ì • íŠ¸ë¦¬ì˜ ì•™ìƒë¸”

--->>>ì•™ìƒë¸” ë°©ë²•ì€ ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì‘ì—…ì—ì„œ ê°€ì¥ ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ë©°

íŠ¹íˆ ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ë¡œ êµ¬ì„±ëœ í‘œ í˜•ì‹ ë°ì´í„°ì—ì„œ ë¹›ì„ ë°œí•¨

ë˜í•œ ì „ì²˜ë¦¬ê°€ ê±°ì˜ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í”„ë¡œí† íƒ€ì…ì„ ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ëŠ” ë° ì í•©

ë§ˆì§€ë§‰ìœ¼ë¡œ íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸°ì™€ ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ê°™ì€ ì•™ìƒë¸” ë°©ë²•ì€ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í•œê³„ê¹Œì§€ ëŒì–´ì˜¬ë¦¬ëŠ”ë° ë„ì›€ì´ ë¨

## 7.1 íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸°

ì§ì ‘ íˆ¬í‘œ ë¶„ë¥˜ê¸°: ê° ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡ì„ ì§‘ê³„í•˜ì—¬ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¡œ ì •í•¨.

-> í° ìˆ˜ì˜ ë²•ì¹™ìœ¼ë¡œ ì¸í•´ ì•½í•œ í•™ìŠµê¸°ê°€ ì¶©ë¶„í•˜ê²Œ ë§ê³  ë‹¤ì–‘í•˜ë©´ ê°•í•œ í•™ìŠµê¸°ê°€ ë  ìˆ˜ ìˆìŒ.

```python
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# ğŸŒŸ 1. moons ë°ì´í„°ì…‹ ìƒì„±
X, y = make_moons(n_samples=500, noise=0.30, random_state=42)

# ğŸŒŸ 2. í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# ğŸŒŸ 3. íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸° ìƒì„± (3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ ì¡°í•©)
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),  # ë¡œì§€ìŠ¤í‹± íšŒê·€
        ('rf', RandomForestClassifier(random_state=42)),  # ëœë¤ í¬ë ˆìŠ¤íŠ¸
        ('svc', SVC(random_state=42))  # ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (SVM)
    ]
)

# ğŸŒŸ 4. ëª¨ë¸ í•™ìŠµ
voting_clf.fit(X_train, y_train)

# ğŸŒŸ 5. í•™ìŠµ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
print("Voting Classifier Training Complete! âœ…")
```

```python
# ê°œë³„ ë¶„ë¥˜ê¸°ì˜ ì •í™•ë„ í‰ê°€
for name, clf in voting_clf.named_estimators_.items():
    print(name, "=", clf.score(X_test, y_test))

# íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡
print(voting_clf.predict(X_test[:1]))

# ê°œë³„ ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])

# íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ í‰ê°€
print(voting_clf.score(X_test, y_test))
```

```python
# ì†Œí”„íŠ¸ íˆ¬í‘œ(Soft Voting) ë°©ì‹ìœ¼ë¡œ ë³€ê²½
voting_clf.voting = "soft"

# SVC ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ í™œì„±í™”
voting_clf.named_estimators_["svc"].probability = True

# ë‹¤ì‹œ ëª¨ë¸ í•™ìŠµ
voting_clf.fit(X_train, y_train)

# íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ í‰ê°€
print(voting_clf.score(X_test, y_test))
```

-> ì˜¤ë¥˜ ìˆ˜ì •

```python
# 1. ìƒˆë¡œìš´ SVC ëª¨ë¸ì„ ìƒì„±í•˜ë©´ì„œ probability=True ì„¤ì •
svc_clf = SVC(probability=True, random_state=42)

# 2. ê¸°ì¡´ voting_clfë¥¼ ìˆ˜ì •í•´ì„œ ìƒˆë¡œìš´ SVC ëª¨ë¸ì„ í¬í•¨í•˜ë„ë¡ ë³€ê²½
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),  
        ('rf', RandomForestClassifier(random_state=42)),  
        ('svc', svc_clf)  # ìˆ˜ì •ëœ SVC ëª¨ë¸ ì¶”ê°€
    ],
    voting="soft"  # ì†Œí”„íŠ¸ íˆ¬í‘œ ë°©ì‹ìœ¼ë¡œ ì„¤ì •
)

# 3. ëª¨ë¸ ì¬í•™ìŠµ
voting_clf.fit(X_train, y_train)

# 4. ì„±ëŠ¥ í‰ê°€
print(voting_clf.score(X_test, y_test))
```

## 7.2 ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ…

ë‹¤ì–‘í•œ ë¶„ë¥˜ê¸° ë§Œë“œëŠ” ë°©ë²•1: ê°ê¸° ë‹¤ë¥¸ í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ

ë‹¤ì–‘í•œ ë¶„ë¥˜ê¸° ë§Œë“œëŠ” ë°©ë²•2: ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê³  í›ˆë ¨ ì„¸íŠ¸ì˜ ì„œë¸Œì…‹ì„ ëœë¤ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë¶„ë¥˜ê¸°ë¥¼ ê°ê¸° ë‹¤ë¥´ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ

ë°°ê¹…: í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ìƒ˜í”Œë§

í˜ì´ìŠ¤íŒ…: í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šê³  ìƒ˜í”Œë§

ì§‘ê³„í•¨ìˆ˜: ë¶„ë¥˜ ì‹œì—ëŠ” í†µê³„ì  ìµœë¹ˆê°’, íšŒê·€ ì‹œì—ëŠ” í‰ê· 

### 7.2.1 ì‚¬ì´í‚·ëŸ°ì˜ ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ…

ë°°ê¹…ì€ ê° ì˜ˆì¸¡ê¸°ê°€ í•™ìŠµí•˜ëŠ” ì„œë¸Œì…‹ì— ë‹¤ì–‘ì„±ì„ ì¶”ê°€í•˜ë¯€ë¡œ ë°°ê¹…ì´ í˜ì´ìŠ¤íŒ…ë³´ë‹¤ í¸í–¥ì´ ì¡°ê¸ˆ ë” ë†’ìŒ.

í•˜ì§€ë§Œ ë‹¤ì–‘ì„±ì„ ì¶”ê°€í•œë‹¤ëŠ” ê²ƒì€ ì˜ˆì¸¡ê¸°ë“¤ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ë¯€ë¡œ ì•™ìƒë¸”ì˜ ë¶„ì‚°ì´ ì¤„ì–´ë“¬.

ì „ë°˜ì ìœ¼ë¡œ ë°°ê¹…ì´ ë” ë‚˜ì€ ëª¨ë¸ì„ ë§Œë“¤ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë” ì„ í˜¸

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# ë°°ê¹… ë¶„ë¥˜ê¸° ìƒì„±
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
    n_estimators=500,      # 500ê°œì˜ ê²°ì • íŠ¸ë¦¬
    max_samples=100,       # ê° íŠ¸ë¦¬ì— 100ê°œì˜ ìƒ˜í”Œ ì‚¬ìš©
    n_jobs=-1,             # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    random_state=42        # ëœë¤ ì‹œë“œ ì„¤ì •
)

# ëª¨ë¸ í•™ìŠµ
bag_clf.fit(X_train, y_train)
```

### 7.2.2 OOB í‰ê°€

OOB(Out Of Bag): ê° ì˜ˆì¸¡ê¸°ì— ì„ íƒë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ í›ˆë ¨ ìƒ˜í”Œ

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# ë°°ê¹… ë¶„ë¥˜ê¸° ìƒì„± (OOB í‰ê°€ í¬í•¨)
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
    n_estimators=500,      # 500ê°œì˜ ê²°ì • íŠ¸ë¦¬ ì‚¬ìš©
    oob_score=True,        # OOB ìƒ˜í”Œì„ ì´ìš©í•œ í‰ê°€ í™œì„±í™”
    n_jobs=-1,             # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    random_state=42        # ëœë¤ ì‹œë“œ ì„¤ì •
)

# ëª¨ë¸ í•™ìŠµ
bag_clf.fit(X_train, y_train)

# OOB í‰ê°€ ì ìˆ˜ ì¶œë ¥
print("OOB Score:", bag_clf.oob_score_)

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì •í™•ë„ í‰ê°€
y_pred = bag_clf.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred))

# OOB ê²°ì • í•¨ìˆ˜ ê°’ í™•ì¸
print("OOB Decision Function (ì²« 3ê°œ ìƒ˜í”Œ):", bag_clf.oob_decision_function_[:3])
```

## 7.3 ëœë¤ íŒ¨ì¹˜ì™€ ëœë¤ ì„œë¸ŒìŠ¤í˜ì´ìŠ¤

ëœë¤ íŒ¨ì¹˜ ë°©ì‹: í›ˆë ¨ íŠ¹ì„±ê³¼ ìƒ˜í”Œì„ ëª¨ë‘ ìƒ˜í”Œë§

ëœë¤ ì„œë¸ŒìŠ¤í˜ì´ìŠ¤ ë°©ì‹: í›ˆë ¨ ìƒ˜í”Œì„ ëª¨ë‘ ì‚¬ìš©í•˜ê³  íŠ¹ì„±ì„ ìƒ˜í”Œë§

## 7.4 ëœë¤ í¬ë ˆìŠ¤íŠ¸

ëœë¤ í¬ë ˆìŠ¤íŠ¸: ì¼ë°˜ì ìœ¼ë¡œ ë°°ê¹… ë°©ë²•(ë˜ëŠ” í˜ì´ìŠ¤íŒ…)ì„ ì ìš©í•œ ê²°ì • íŠ¸ë¦¬ì˜ ì•™ìƒë¸”

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ìƒì„±
rnd_clf = RandomForestClassifier(
    n_estimators=500,  # 500ê°œì˜ íŠ¸ë¦¬ ì‚¬ìš©
    max_leaf_nodes=16,  # ìµœëŒ€ ë¦¬í”„ ë…¸ë“œ ê°œìˆ˜ ì œí•œ
    n_jobs=-1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    random_state=42  # ëœë¤ ì‹œë“œ ì„¤ì •
)

# ëª¨ë¸ í•™ìŠµ
rnd_clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ìˆ˜í–‰
y_pred_rf = rnd_clf.predict(X_test)

# ë°°ê¹…ì„ ì´ìš©í•œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ë™ì¼í•œ ëª¨ë¸ ìƒì„±
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
    n_estimators=500,  # 500ê°œì˜ íŠ¸ë¦¬ ì‚¬ìš©
    n_jobs=-1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    random_state=42  # ëœë¤ ì‹œë“œ ì„¤ì •
)

# ë°°ê¹… ëª¨ë¸ í•™ìŠµ
bag_clf.fit(X_train, y_train)
```

### 7.4.1 ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬

ê° ë…¸ë“œëŠ” ëœë¤ìœ¼ë¡œ íŠ¹ì„±ì˜ ì„œë¸Œì…‹ì„ ë§Œë“¤ì–´ ë¶„í• ì— ì‚¬ìš©

ìµìŠ¤íŠ¸ë¦¼ ëœë¤ íŠ¸ë¦¬(=ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬): ê·¹ë‹¨ì ìœ¼ë¡œ ëœë¤í•œ íŠ¸ë¦¬, í¸í–¥ì´ ëŠ˜ì–´ë‚˜ëŠ” ëŒ€ì‹  ë¶„ì‚°ì´ ë‚®ì•„ì§

### 7.4.2 íŠ¹ì„± ì¤‘ìš”ë„

ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì¥ì : íŠ¹ì„±ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì¸¡ì •í•˜ê¸° ì‰¬ì›€

-> ì–´ë–¤ íŠ¹ì„±ì„ ì‚¬ìš©í•œ ë…¸ë“œê°€ í‰ê· ì ìœ¼ë¡œ ë¶ˆìˆœë„ë¥¼ ì–¼ë§ˆë‚˜ ê°ì†Œì‹œí‚¤ëŠ”ì§€ í™•ì¸í•˜ì—¬ íŠ¹ì„±ì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì •

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

# ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ì…‹ ë¡œë“œ
iris = load_iris(as_frame=True)

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)
rnd_clf.fit(iris.data, iris.target)

# íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):
    print(round(score, 2), name)
```

## 7.5 ë¶€ìŠ¤íŒ…

ë¶€ìŠ¤íŒ…(=ê°€ì„¤ ë¶€ìŠ¤íŒ…): ì•½í•œ í•™ìŠµê¸°ë¥¼ ì—¬ëŸ¬ ê°œ ì—°ê²°í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°ë¥¼ ë§Œë“œëŠ” ì•™ìƒë¸” ë°©ë²•

ì•ì˜ ëª¨ë¸ì„ ë³´ì™„í•´ ë‚˜ê°€ë©´ì„œ ì¼ë ¨ì˜ ì˜ˆì¸¡ê¸°ë¥¼ í•™ìŠµì‹œí‚´

### 7.5.1 AdaBoost

ì´ì „ ëª¨ë¸ì´ ê³¼ì†Œì í•©í–ˆë˜ í›ˆë ¨ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ë¥¼ ë” ë†’ì„

-> ìƒˆë¡œìš´ ì˜ˆì¸¡ê¸°ëŠ” í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ìƒ˜í”Œì— ì ì  ë” ë§ì¶°ì§€ê²Œ ë¨

ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í›ˆë ¨ ì„¸íŠ¸ì˜ ì „ë°˜ì ì¸ ì •í™•ë„ì— ë”°ë¼ ì˜ˆì¸¡ê¸°ë§ˆë‹¤ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ê°€ ì ìš©ë¨

![image](https://github.com/user-attachments/assets/500bbcec-b48b-4ab8-b4bc-fbbcc6c10105)

![image](https://github.com/user-attachments/assets/3740491b-bf1b-4d1e-a7e5-180a8d0c131d)

![image](https://github.com/user-attachments/assets/8f0332f4-a299-47a7-a223-9bc4eafe5cf3)

![image](https://github.com/user-attachments/assets/28ea13cd-e53a-45b8-b67f-49bba8dcd6c4)

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# AdaBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1),  # ì•½í•œ í•™ìŠµê¸° (ê¹Šì´ê°€ 1ì¸ ê²°ì • íŠ¸ë¦¬)
    n_estimators=30,  # íŠ¸ë¦¬ ê°œìˆ˜
    learning_rate=0.5,  # í•™ìŠµë¥ 
    random_state=42
)

# ëª¨ë¸ í•™ìŠµ
ada_clf.fit(X_train, y_train)
```

### 7.5.2 ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…

ì´ì „ ì˜ˆì¸¡ê¸°ê°€ ë§Œë“  ì”ì—¬ ì˜¤ì°¨ì— ìƒˆë¡œìš´ ì˜ˆì¸¡ê¸°ë¥¼ í•™ìŠµ

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor

# ë‚œìˆ˜ ì´ˆê¸°í™”
np.random.seed(42)

# ë°ì´í„° ìƒì„±
X = np.random.rand(100, 1) - 0.5
y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3xÂ² + ê°€ìš°ìŠ¤ ì¡ìŒ

# ì²« ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ íšŒê·€ ëª¨ë¸
tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)

# ì²« ë²ˆì§¸ ì˜ˆì¸¡ê°’ ìƒì„±
y2 = y - tree_reg1.predict(X)

# ë‘ ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ íšŒê·€ ëª¨ë¸
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)
tree_reg2.fit(X, y2)

# ë‘ ë²ˆì§¸ ì˜ˆì¸¡ê°’ ìƒì„±
y3 = y2 - tree_reg2.predict(X)

# ì„¸ ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ íšŒê·€ ëª¨ë¸
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)
tree_reg3.fit(X, y3)

# ìƒˆë¡œìš´ ìƒ˜í”Œ ì˜ˆì¸¡
X_new = np.array([[-0.4], [0.], [0.5]])
predictions = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
print(predictions)
```

```python
from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boosting Regressor ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,
                                 learning_rate=1.0, random_state=42)
gbrt.fit(X, y)
```

ì¶•ì†Œ: ê¸°ì—¬ë„ë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ëŠ” ê·œì œ ë°©ë²• 

-> ì•™ìƒë¸”ì„ í›ˆë ¨ ì„¸íŠ¸ì— í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ë§ì€ íŠ¸ë¦¬ê°€ í•„ìš”í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì˜ˆì¸¡ì˜ ì„±ëŠ¥ì€ ì¢‹ì•„ì§

'''python
from sklearn.ensemble import GradientBoostingRegressor

gbrt_best = GradientBoostingRegressor(
    max_depth=2,
    learning_rate=0.05,
    n_estimators=500,
    n_iter_no_change=10,
    random_state=42
)

gbrt_best.fit(X, y)

print(gbrt_best.n_estimators_) # ìµœì ì˜ íŠ¸ë¦¬ ê°œìˆ˜ í™•ì¸
'''

í™•ë¥ ì  ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…: ê° íŠ¸ë¦¬ê°€ í›ˆë ¨í•  ë•Œ ì‚¬ìš©í•  í›ˆë ¨ ìƒ˜í”Œì˜ ë¹„ìš©ì„ ì§€ì •

### 7.5.3 íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…

ì…ë ¥ íŠ¹ì„±ì„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì •ìˆ˜ë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™

êµ¬ê°„ ë¶„í• ì„ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ í‰ê°€í•´ì•¼ í•˜ëŠ” ì„ê³—ê°’ì˜ ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒ

ë˜í•œ ì •ìˆ˜ë¡œ ì‘ì—…í•˜ë©´ ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

ì •ë°€ë„ ì†ì‹¤ì„ ìœ ë°œí•˜ë¯€ë¡œ ë°ì´í„°ì…‹ì— ë”°ë¼ ê³¼ëŒ€ì í•©ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ë„ ê³¼ì†Œì í•©ì„ ìœ ë°œí•  ìˆ˜ë„ ìˆìŒ

```python
from sklearn.datasets import fetch_california_housing
import pandas as pd

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
housing_data = fetch_california_housing(as_frame=True)
housing = housing_data.frame
housing_labels = housing["MedHouseVal"]  # íƒ€ê¹ƒ ë³€ìˆ˜ (ì§‘ ê°’)
housing = housing.drop("MedHouseVal", axis=1)  # ì…ë ¥ ë³€ìˆ˜ë§Œ ë‚¨ê¹€

# ocean_proximity ë³€ìˆ˜ë¥¼ ê°€ì • (ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ë°ì´í„°ì—ëŠ” ì—†ìŒ, ì˜ˆì œìš© ì¶”ê°€)
housing["ocean_proximity"] = pd.cut(housing["AveRooms"],
                                    bins=[0, 2, 5, 10, float("inf")],
                                    labels=["near", "mid", "far", "very_far"])


from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.preprocessing import OrdinalEncoder

hgb_reg = make_pipeline(
    make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]),
                            remainder="passthrough"),
    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)
)

hgb_reg.fit(housing, housing_labels)
```

## 7.6 ìŠ¤íƒœí‚¹

ì•™ìƒë¸”ì— ì†í•œ ëª¨ë“  ì˜ˆì¸¡ê¸°ì˜ ì˜ˆì¸¡ì„ ì·¨í•©í•˜ëŠ” ëª¨ë¸

ë¸”ë Œë”(=ë©”íƒ€ í•™ìŠµê¸°): ë§ˆì§€ë§‰ ì˜ˆì¸¡ê¸°

![image](https://github.com/user-attachments/assets/175a8871-d460-4b59-9e2a-1be89bd73b1f)

![image](https://github.com/user-attachments/assets/d40a9df4-314a-443e-9e2c-83dc31fef369)

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

stacking_clf = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),
        ('rf', RandomForestClassifier(random_state=42)),
        ('svc', SVC(probability=True, random_state=42))
    ],
    final_estimator=RandomForestClassifier(random_state=43),
    cv=5  # êµì°¨ ê²€ì¦ í´ë“œ ê°œìˆ˜
)

stacking_clf.fit(X_train, y_train)
```
