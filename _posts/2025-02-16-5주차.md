---
layout : post
title : "5주차"
---

# 8장 차원 축소

## 8.1 차원의 저주

우리는 3차원 세계에 살고 있어 고차원 공간을 직관적으로 상상하기 어려움

고차원 공간에서는 많은 것이 상당히 다르게 작동하는데 

예를 들어, 단위 면적에 있는 점을 랜덤으로 선택할 때 이것이 경계선에서 0.001 이내에 위치할 가능성은 0.4%지만,

10,000 차원의 단위 면적을 가진 초입방체에서는 그 가능성이 99.99%보다 커짐

즉, 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있음

그리고 고차원은 많은 공간을 가지고 있어 두 점 사이의 거리가 매우 멈

이는 고차원 데이터셋은 매우 희박할 위험이 있어 새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높음

간단히 말해 훈련 세트의 차원이 클수록 과대적합 위험이 커지는 차원의 저주에 걸림

이를 해결하기 위해 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키워야함

## 8.2 차원 축소를 위한 접근법

### 8.2.1 투영

많은 특성은 거의 변화가 없는 반면, 다른 특성들은 서로 강하게 연관되어 있음

결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 놓여 있음

![image](https://github.com/user-attachments/assets/f432cc52-34f2-4495-b6cf-2d1fcfef06bf)

이때 투영을 사용하면 차원 축소가 가능

![image](https://github.com/user-attachments/assets/9f3a111e-426b-4738-9fd5-82ce848d6e2d)

하지만 스위스 롤 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있는 경우

![image](https://github.com/user-attachments/assets/1bc172fa-2767-46e7-a15a-efd3325d2c36)

그냥 평면에 투영시키면 스위스 롤의 층이 서로 뭉개질 수 있음

![image](https://github.com/user-attachments/assets/a654b289-016e-4952-8a47-3ab4f21dcc7a)

### 8.2.2 매니폴드 학습

스위스 롤은 2D 매니폴드의 한 예로, 2D 매니폴드는 고차원 공간에서 휘어지거나 뒤틀린 2D 모양

더 일반적으로 d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부(d<n)

매니폴드 학습: 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링

매니폴드 가정(=매니폴드 가설): 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여있음

처리해야 할 작업이 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이란 가정과 종종 병행됨

하지만 이런 암묵적인 가정이 항상 유효하지는 않고, 데이터셋에 따라 다름

![image](https://github.com/user-attachments/assets/72741670-26b9-4c9d-9054-b47926e0110b)

## 8.3 주성분 분석

주성분 분석(PCA): 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 이 평며에 투영시키는 가장 인기 있는 차원 축소 알고리즘

### 8.3.1 분산 보존

![image](https://github.com/user-attachments/assets/7164b2af-f057-48e7-bad8-668193bfdc33)

저차원의 초평면에 훈련 세트를 투영하기 전에 먼저 올바른 초평면을 선택해야함

다른 방향으로 투영하는 것보다 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적

### 8.3.2 주성분







