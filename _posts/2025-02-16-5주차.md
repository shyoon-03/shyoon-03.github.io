---
layout : post
title : "5주차"
---

# 8장 차원 축소

## 8.1 차원의 저주

우리는 3차원 세계에 살고 있어 고차원 공간을 직관적으로 상상하기 어려움

고차원 공간에서는 많은 것이 상당히 다르게 작동하는데 

예를 들어, 단위 면적에 있는 점을 랜덤으로 선택할 때 이것이 경계선에서 0.001 이내에 위치할 가능성은 0.4%지만,

10,000 차원의 단위 면적을 가진 초입방체에서는 그 가능성이 99.99%보다 커짐

즉, 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있음

그리고 고차원은 많은 공간을 가지고 있어 두 점 사이의 거리가 매우 멈

이는 고차원 데이터셋은 매우 희박할 위험이 있어 새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높음

간단히 말해 훈련 세트의 차원이 클수록 과대적합 위험이 커지는 차원의 저주에 걸림

이를 해결하기 위해 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키워야함

## 8.2 차원 축소를 위한 접근법

### 8.2.1 투영

많은 특성은 거의 변화가 없는 반면, 다른 특성들은 서로 강하게 연관되어 있음

결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 놓여 있음

![image](https://github.com/user-attachments/assets/f432cc52-34f2-4495-b6cf-2d1fcfef06bf)

이때 투영을 사용하면 차원 축소가 가능

![image](https://github.com/user-attachments/assets/9f3a111e-426b-4738-9fd5-82ce848d6e2d)

하지만 스위스 롤 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있는 경우

![image](https://github.com/user-attachments/assets/1bc172fa-2767-46e7-a15a-efd3325d2c36)

그냥 평면에 투영시키면 스위스 롤의 층이 서로 뭉개질 수 있음

![image](https://github.com/user-attachments/assets/a654b289-016e-4952-8a47-3ab4f21dcc7a)

### 8.2.2 매니폴드 학습

스위스 롤은 2D 매니폴드의 한 예로, 2D 매니폴드는 고차원 공간에서 휘어지거나 뒤틀린 2D 모양

더 일반적으로 d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부(d<n)

매니폴드 학습: 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링

매니폴드 가정(=매니폴드 가설): 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여있음

처리해야 할 작업이 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이란 가정과 종종 병행됨

하지만 이런 암묵적인 가정이 항상 유효하지는 않고, 데이터셋에 따라 다름

![image](https://github.com/user-attachments/assets/72741670-26b9-4c9d-9054-b47926e0110b)

## 8.3 주성분 분석

주성분 분석(PCA): 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 이 평며에 투영시키는 가장 인기 있는 차원 축소 알고리즘

### 8.3.1 분산 보존

![image](https://github.com/user-attachments/assets/7164b2af-f057-48e7-bad8-668193bfdc33)

저차원의 초평면에 훈련 세트를 투영하기 전에 먼저 올바른 초평면을 선택해야함

다른 방향으로 투영하는 것보다 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적

### 8.3.2 주성분

첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는 방식을 계속 반복

이때 i번째 축을 이 데이터의 i번째 주성분(PC)라 부름

특잇값 분해(SVD)로 훈련 세트 행렬 X를 분해하면, 찾고자 하는 모든 주성분의 단위 벡터가 V에 담겨있음

![image](https://github.com/user-attachments/assets/757efe1e-15db-4618-af11-dc5f5b3745b3)

```python
import numpy as np

# 3D 데이터셋 예제 (3차원 데이터, 10개 샘플)
X = np.array([
    [2.5, 2.4, 1.5],
    [0.5, 0.7, 0.9],
    [2.2, 2.9, 1.3],
    [1.9, 2.2, 1.8],
    [3.1, 3.0, 2.5],
    [2.3, 2.7, 2.1],
    [2.0, 1.6, 1.7],
    [1.0, 1.1, 0.8],
    [1.5, 1.6, 1.0],
    [1.1, 0.9, 0.6]
])

# 데이터의 평균을 중심으로 이동
X_centered = X - X.mean(axis=0)

# 특이값 분해(SVD) 수행
U, s, Vt = np.linalg.svd(X_centered)

# 첫 번째와 두 번째 주성분 벡터
c1 = Vt[0]
c2 = Vt[1]

print("첫 번째 주성분 벡터:", c1)
print("두 번째 주성분 벡터:", c2)
```

### 8.3.3 d차원으로 투영하기

주성분을 모두 추출 후 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소

이 초평면은 분산을 가능한 한 최대로 보존하는 투영임

![image](https://github.com/user-attachments/assets/d0bd41d6-b13d-4505-a916-ad22a68c7de7)

```python
W2 = Vt[:2].T
X2D = X_centered @ W2
```

### 8.3.4 사이킷런 사용하기

사이킷런의 PCA 모델은 SVD를 사용하여 구현(자동으로 데이터를 중앙에 맞춰줌)

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```

### 8.3.5 설명된 분산의 비율

변수에 저장된 주성분의 '설명된 분산의 비율': 각 주성분의 축을 따라 있는 데이터셋의 분산 비율

```python
pca.explained_variance_ratio_
```

### 8.3.6 적절한 차원 수 선택

방법 1

데이터 시각화를 위해 차원을 2개나 3개로 줄이는 경우를 제외하고는 축소할 차원 수를 임의로 정하기보다는 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 간단함

```python
from sklearn.datasets import fetch_openml
import numpy as np
from sklearn.decomposition import PCA

# MNIST 데이터셋 로드
mnist = fetch_openml('mnist_784', as_frame=False)

# 데이터 분할
X_train, y_train = mnist.data[:60000], mnist.target[:60000]
X_test, y_test = mnist.data[60000:], mnist.target[60000:]

# PCA 적용
pca = PCA()
pca.fit(X_train)

# 누적 분산 비율 계산
cumsum = np.cumsum(pca.explained_variance_ratio_)

# 필요한 주성분 개수 찾기 (설명된 분산이 95% 이상)
d = np.argmax(cumsum > 0.95) + 1  # d = 154

# 새로운 PCA 모델 적용 (95% 분산 유지)
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)

pca.n_components_
```

방법 2

![image](https://github.com/user-attachments/assets/1abcaa00-8e97-45cf-ada1-edd9113e60f5)

방법 3

지도 학습 작업의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝 가능

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
import numpy as np

# 파이프라인 생성 (PCA + 랜덤 포레스트)
clf = make_pipeline(
    PCA(random_state=42),
    RandomForestClassifier(random_state=42)
)

# 하이퍼파라미터 분포 정의
param_distrib = {
    "pca__n_components": np.arange(10, 80),
    "randomforestclassifier__n_estimators": np.arange(50, 500)
}

# 랜덤 서치 CV 수행
rnd_search = RandomizedSearchCV(
    clf, param_distributions=param_distrib, n_iter=10, cv=3, random_state=42
)

# 데이터셋 일부만 사용하여 학습
rnd_search.fit(X_train[:1000], y_train[:1000])

# 최적 하이퍼파라미터 출력
print(rnd_search.best_params_)
# {'randomforestclassifier__n_estimators': 465, 'pca__n_components': 23}
```

### 8.3.7 압축을 위한 PCA

차원 축소 후 데이터셋의 크기가 준 것에 비해 분산의 손실이 적은 편

이러한 압축률은 분류 알고리즘의 속도를 크게 높일 수 있음

또한 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용하여 차원을 되돌리기 가능

재구성 오차: 원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리

```python
# PCA로 변환된 데이터 복원
X_recovered = pca.inverse_transform(X_reduced)
```

![image](https://github.com/user-attachments/assets/8bcd7b5a-f1cf-4aba-b790-cca101e27983)

![image](https://github.com/user-attachments/assets/98c7a8a1-661b-4bdc-ad07-d53df52899bf)

### 8.3.8 랜덤 PCA

svd_solver="randomized"의 계산 복잡도: 원래 SVD 방식에서 n 대신 d -> d가 n보다 많이 작을수록 훨씬 빨라짐

```python
from sklearn.decomposition import PCA

# PCA 적용 (랜덤 SVD 솔버 사용)
rnd_pca = PCA(n_components=154, svd_solver="randomized", random_state=42)
X_reduced = rnd_pca.fit_transform(X_train)
```

### 8.3.9 점진적 PCA

점진적 PCA(IPCA): 전체 훈련 세트를 메모리 올려야 하는 단점을 보완하여 훈련 세트를 미니배치로 나눈 뒤 IPCA 알고리즘에 한 번에 하나씩 주입

```python
from sklearn.decomposition import IncrementalPCA
import numpy as np

# Incremental PCA 설정
n_batches = 100
inc_pca = IncrementalPCA(n_components=154)

# 배치 단위로 Partial Fit 수행
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)

# 전체 데이터 변환
X_reduced = inc_pca.transform(X_train)

# 메모리 매핑 파일(mmap) 생성 및 저장
filename = "my_mnist.mmap"
X_mmap = np.memmap(filename, dtype="float32", mode="write", shape=X_train.shape)
X_mmap[:] = X_train  # 데이터를 메모리 매핑 파일에 저장
X_mmap.flush()  # 파일에 변경 사항 저장

# 메모리 매핑 파일을 읽어서 Incremental PCA 적용
X_mmap = np.memmap(filename, dtype="float32", mode="readonly").reshape(-1, 784)
batch_size = X_mmap.shape[0] // n_batches

inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mmap)
```

매우 고차원인 데이터 셋의 경우 계산 복잡도는 여전히 d가 좌우하므로 목표 차원 수 d가 너무 크지 않아야 함

## 8.4 랜덤 투영

랜덤 투영 알고리즘은 랜덤한 선형 투영을 사용하여 데이터를 저차원 공간에 투영함

이는 실제로 거리를 상당히 잘 보존할 가능성이 매우 높음

더 많은 차원을 삭제할수록 더 많은 정보가 손실되고 더 많은 거리가 왜곡되어 주어진 허용 오차 이상으로 변하지 않도록 보장하기 위해 보존할 최소 차원 수를 결정

```python
from sklearn.random_projection import johnson_lindenstrauss_min_dim
import numpy as np

# Johnson-Lindenstrauss 최소 차원 계산
m, eps = 5000, 0.1
d = johnson_lindenstrauss_min_dim(m, eps=eps)

print(d)  # 출력: 7300

# 가우스 분포를 따르는 랜덤 투영 행렬 P 생성
n = 20000
np.random.seed(42)
P = np.random.randn(d, n) / np.sqrt(d)  # 표준 편차 = 분산의 제곱근

# 가짜 데이터셋 생성 및 차원 축소
X = np.random.randn(m, n)
X_reduced = X @ P.T
```

```python
from sklearn.random_projection import GaussianRandomProjection

# 가우시안 랜덤 프로젝션 생성 및 적용
gaussian_rnd_proj = GaussianRandomProjection(eps=eps, random_state=42)
X_reduced = gaussian_rnd_proj.fit_transform(X)  # 이전 결과와 동일
```

밀도: 희소한 랜덤 행렬에서 0이 아닌 항목의 비율 r

```python
# 역변환을 위한 유사 역행렬 계산
components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)
X_recovered = X_reduced @ components_pinv.T  # 차원 축소된 데이터를 원래 차원으로 변환
```

## 8.5 지역 선형 임베딩

지역 선형 임베딩(LLE)는 비선형 차원 축소(NLDR) 기술임

투영에 의존하지 않는 매니폴드 학습으로 

각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정한 다음

국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾음

```python
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

# 스위스 롤 데이터셋 생성
X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)

# 국소적 선형 임베딩(LLE) 적용
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_unrolled = lle.fit_transform(X_swiss)
```

![image](https://github.com/user-attachments/assets/8e15f556-20e0-4413-ad7a-76e136531880)

![image](https://github.com/user-attachments/assets/48911bcb-2bf0-4c73-8797-5ec6be7bdffd)

![image](https://github.com/user-attachments/assets/aa3c6792-ac14-459b-ac14-045336819351)

데이터가 비선형일 경우 훨씬 더 나은 저차원 표현을 구성 가능

## 8.6 다른 차원 축소 기법

#### 다차원 스케일링

샘플 간의 거리를 보존하며 차원을 축소함

랜덤 투영은 고차원 데이터에는 적합하지만 저차원 데이터에는 잘 작동하지 않음

#### Isomap

각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만듬

그런 다음 샘플 간의 지오데식 거리를 유지하면서 차원을 축소

그래프에서 두 노드 사이의 지오데식 거리는 두 노드 사이의 최단 경로를 이루는 노드의 수

#### t-SNE

비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소

주로 시각화에 많이 사용되며 특히 고차원 공간에 있는 샘플의 군집을 시각화할 때 사용

#### 선형 판별 분석

선형 분류 알고리즘으로 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습

이 축은 데이터가 투영되는 초평면을 정의하는 데 사용

투영을 통해 가능한 한 클래스를 멀리 떨어지게 유지시켜 다른 분류 알고리즘 적용 전 차원 축소에 좋음

![image](https://github.com/user-attachments/assets/e7df93e7-a349-47b7-bdc9-424719c05d18)
